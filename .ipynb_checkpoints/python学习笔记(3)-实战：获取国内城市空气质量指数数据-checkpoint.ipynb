{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float:center;\" src=\"./img/aqi_review.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/><br/>\n",
    "# 对空气质量历史数据的爬取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<font size=4>**1、创建工程，在命令行终端创建一个名为air_quality的工程,并进入该工程目录**</font>  \n",
    "<br/>\n",
    "&emsp;&emsp;&emsp;&emsp;<font size=4 color='red'>*c:>scrapy startproject air_quality*</font>\n",
    "<br/>\n",
    "&emsp;&emsp;&emsp;&emsp;<font size=4 color='red'>*c:>cd air_quality</font>*  \n",
    "<br/>\n",
    "&emsp;&emsp;<font size=4>**2、编写spider**</font>  \n",
    "<br/>\n",
    "&emsp;&emsp;&emsp;&emsp;<font size=4 color='red'>*c:\\air_quality>scrapy genspider air_history_spider https://www.aqistudy.cn/historydata/index.php*</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>**文件目录如图所示**</font>  \n",
    "<img style=\"float:left;\" src=\"./img/aqi_filefolder.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seetings.py\n",
    "\n",
    "ITEM_PIPELINES = {\n",
    "   'air_quality.pipelines.AirQualityPipeline': 300,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# items.py\n",
    "\n",
    "import scrapy\n",
    "\n",
    "class AirQualityItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    # name = scrapy.Field()\n",
    "    city_name = scrapy.Field()      # 城市名称\n",
    "    record_date = scrapy.Field()    # 检测日期\n",
    "    aqi_val = scrapy.Field()        # AQI\n",
    "    range_val = scrapy.Field()      # 范围\n",
    "    quality_level = scrapy.Field()  # 质量等级\n",
    "    pm2_5_val = scrapy.Field()      # PM2.5\n",
    "    pm10_val = scrapy.Field()       # PM10\n",
    "    so2_val = scrapy.Field()        # SO2\n",
    "    co_val = scrapy.Field()         # CO\n",
    "    no2_val = scrapy.Field()        # NO2\n",
    "    o3_val = scrapy.Field()         # O3\n",
    "    rank = scrapy.Field()           # 排名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipelines.py\n",
    "\n",
    "from scrapy.exporters import CsvItemExporter\n",
    "\n",
    "class AirQualityPipeline(object):\n",
    "\n",
    "    def open_spider(self,spider):\n",
    "        self.file = open('air_quality.csv', 'wb')\n",
    "        self.exporter = CsvItemExporter(self.file)\n",
    "        self.exporter.start_exporting()\n",
    "\n",
    "    def close_spider(self,spider):\n",
    "        self.exporter.finish_exporting()\n",
    "        self.file.close()\n",
    "\n",
    "    def process_item(self, item,spider):\n",
    "        self.exporter.export_item(item)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api_history_spider.py\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "from urllib import parse\n",
    "from air_quality.items import AirQualityItem\n",
    "\n",
    "base_url = 'https://www.aqistudy.cn/historydata/'\n",
    "\n",
    "class ApiHistorySpiderSpider(scrapy.Spider):\n",
    "    name = 'api_history_spider'\n",
    "    allowed_domains = [\"aqistudy.cn\"]\n",
    "    start_urls = ['https://www.aqistudy.cn/historydata/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        \"\"\"\n",
    "            解析初始页面\n",
    "        \"\"\"\n",
    "        # 获取所有城市的URL\n",
    "        city_url_list = response.xpath('//div[@class=\"all\"]//div[@class=\"bottom\"]//a//@href')\n",
    "\n",
    "        for city_url in city_url_list:\n",
    "            # 依次遍历城市URL\n",
    "            city_month_url = base_url + city_url.extract()\n",
    "            # 解析每个城市的月份数据\n",
    "            request = scrapy.Request(city_month_url, callback=self.parse_city_month)\n",
    "            yield request\n",
    "\n",
    "    def parse_city_month(self, response):\n",
    "        \"\"\"\n",
    "            解析该城市的月份数据\n",
    "        \"\"\"\n",
    "        # 获取该城市的所有月份URL\n",
    "        month_url_list = response.xpath('//table[@class=\"table table-condensed '\n",
    "                                        'table-bordered table-striped table-hover '\n",
    "                                        'table-responsive\"]//a//@href')\n",
    "\n",
    "        for month_url in month_url_list:\n",
    "            # 依次遍历月份URL\n",
    "            city_day_url = base_url + month_url.extract()\n",
    "            # 解析该城市的每日数据\n",
    "            request = scrapy.Request(city_day_url, callback=self.parse_city_day)\n",
    "            yield request\n",
    "\n",
    "    def parse_city_day(self, response):\n",
    "        \"\"\"\n",
    "            解析该城市的每日数据\n",
    "        \"\"\"\n",
    "        url = response.url\n",
    "        item = AirQualityItem()\n",
    "        city_url_name = url[url.find('=') + 1:url.find('&')]\n",
    "\n",
    "        # 解析url中文\n",
    "        # item['city_name'] = city_url_name\n",
    "        item['city_name'] = parse.unquote(city_url_name)\n",
    "\n",
    "        # 获取每日记录\n",
    "        day_record_list = response.xpath('//table[@class=\"table table-condensed '\n",
    "                                         'table-bordered table-striped table-hover '\n",
    "                                         'table-responsive\"]//tr')\n",
    "        for i, day_record in enumerate(day_record_list):\n",
    "            if i == 0:\n",
    "                # 跳过表头\n",
    "                continue\n",
    "            td_list = day_record.xpath('.//td')\n",
    "\n",
    "            item['record_date'] = td_list[0].xpath('text()').extract_first()  # 检测日期\n",
    "            item['aqi_val'] = td_list[1].xpath('text()').extract_first()  # AQI\n",
    "            item['range_val'] = td_list[2].xpath('text()').extract_first()  # 范围\n",
    "            item['quality_level'] = td_list[3].xpath('.//div/text()').extract_first()  # 质量等级\n",
    "            item['pm2_5_val'] = td_list[4].xpath('text()').extract_first()  # PM2.5\n",
    "            item['pm10_val'] = td_list[5].xpath('text()').extract_first()  # PM10\n",
    "            item['so2_val'] = td_list[6].xpath('text()').extract_first()  # SO2\n",
    "            item['co_val'] = td_list[7].xpath('text()').extract_first()  # CO\n",
    "            item['no2_val'] = td_list[8].xpath('text()').extract_first()  # NO2\n",
    "            item['o3_val'] = td_list[9].xpath('text()').extract_first()  # O3\n",
    "            item['rank'] = td_list[10].xpath('text()').extract_first()  # 排名\n",
    "\n",
    "            yield item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;<font size=4>**3、运行spider**</font>  \n",
    "<br/>\n",
    "&emsp;&emsp;&emsp;&emsp;<font size=4 color='red'>*scrapy crawl api_history_spider*</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4>**得到csv文件，部分如下图所示：**</font>  \n",
    "<img style=\"float:left;\" src=\"./img/aqi_result.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
